{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'transformer'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m os\u001b[38;5;241m.\u001b[39mchdir(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/n/home11/tanishqkumar/gravity-chamber/fundamentals/llm\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformer\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Transformer\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtqdm\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m tqdm\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'transformer'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "sys.path.append(\"../train-vanilla-transformer/\")\n",
    "import torch\n",
    "from transformer import Transformer\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Initialize model parameters\n",
    "vocab_size = 256  # Using a small vocab size for testing\n",
    "hidden_dim = 512\n",
    "n_layers = 6\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# Create model\n",
    "model = Transformer(depth=n_layers, hidden_dim=hidden_dim, vocab_size=vocab_size, device=device)\n",
    "model = model.to(device)\n",
    "model.eval()  # Set to evaluation mode\n",
    "\n",
    "# Generate sample input\n",
    "batch_size = 1\n",
    "seq_len = 512\n",
    "x = torch.randint(0, vocab_size, (batch_size, seq_len)).to(device)\n",
    "\n",
    "# Run inference\n",
    "with torch.no_grad():\n",
    "    out = model(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'transformer'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 17\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;66;03m# Optionally, set a default path or raise an error if the path is critical\u001b[39;00m\n\u001b[1;32m     14\u001b[0m     \u001b[38;5;66;03m# os.chdir(\".\") # Example: change to current directory if the target doesn't exist\u001b[39;00m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m---> 17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformer\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Transformer\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgeneration\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m TransformerGenerator \u001b[38;5;66;03m# Import the KV cache generator\u001b[39;00m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'transformer'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "import torch\n",
    "from transformer import Transformer\n",
    "from generation import TransformerGenerator # Import the KV cache generator\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "# Initialize model parameters\n",
    "vocab_size = 256  # Using a small vocab size for testing\n",
    "hidden_dim = 512\n",
    "n_layers = 6\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# Create model\n",
    "model = Transformer(depth=n_layers, hidden_dim=hidden_dim, vocab_size=vocab_size, device=device)\n",
    "model = model.to(device)\n",
    "model.eval()  # Set to evaluation mode\n",
    "\n",
    "# Create KV cache generator instance\n",
    "generator = TransformerGenerator(model)\n",
    "\n",
    "# Profile decoding across different numbers of tokens to generate\n",
    "input_seq_len = 512  # Fixed input sequence length\n",
    "num_decode_tokens = [512, 2048, 4096, 8192]  # Number of tokens to decode\n",
    "decode_times_no_kv = []\n",
    "decode_times_kv = []\n",
    "n_runs = 1\n",
    "\n",
    "with torch.no_grad():\n",
    "    for n_tokens in tqdm(num_decode_tokens):\n",
    "        batch_size = 1\n",
    "        times_for_this_n_no_kv = []\n",
    "        times_for_this_n_kv = []\n",
    "\n",
    "        for _ in range(n_runs):\n",
    "            # Create input sequence (prompt)\n",
    "            prompt_tensor = torch.randint(0, vocab_size, (batch_size, input_seq_len)).to(device)\n",
    "            prompt_list = prompt_tensor.squeeze(0).tolist() # For KV cache generator\n",
    "\n",
    "            # --- Time the decoding WITHOUT KV Cache ---\n",
    "            x = prompt_tensor.clone() # Use a copy for the non-KV version\n",
    "            start_time_no_kv = time.time()\n",
    "            # Autoregressive decoding (no KV cache)\n",
    "            for _ in range(n_tokens):\n",
    "                logits = model(x)  # Get next token logits\n",
    "                # logits is [B, S, V] from model.unemb, logits[:, -1:] is [B, 1, V]\n",
    "                next_token = torch.argmax(logits[:, -1:, :], dim=-1)  # [B, 1]\n",
    "                x = torch.cat([x, next_token], dim=1)  # Append to sequence\n",
    "            torch.cuda.synchronize()  # Wait for CUDA operations to complete\n",
    "            end_time_no_kv = time.time()\n",
    "            times_for_this_n_no_kv.append((end_time_no_kv - start_time_no_kv) * 1000)  # Convert to ms\n",
    "\n",
    "            # --- Time the decoding WITH KV Cache ---\n",
    "            start_time_kv = time.time()\n",
    "            # Autoregressive decoding (with KV cache) using TransformerGenerator\n",
    "            _ = generator.generate(prompt_list, n_tokens)\n",
    "            torch.cuda.synchronize() # Wait for CUDA operations to complete\n",
    "            end_time_kv = time.time()\n",
    "            times_for_this_n_kv.append((end_time_kv - start_time_kv) * 1000) # Convert to ms\n",
    "\n",
    "        # Take average across runs\n",
    "        avg_time_no_kv = np.mean(times_for_this_n_no_kv)\n",
    "        decode_times_no_kv.append(avg_time_no_kv)\n",
    "        avg_time_kv = np.mean(times_for_this_n_kv)\n",
    "        decode_times_kv.append(avg_time_kv)\n",
    "\n",
    "\n",
    "# Plot results\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(num_decode_tokens, decode_times_no_kv, marker='o', label='No KV Cache')\n",
    "plt.plot(num_decode_tokens, decode_times_kv, marker='s', label='With KV Cache')\n",
    "plt.xlabel('Number of Decoded Tokens')\n",
    "plt.ylabel('Total Time (ms)')\n",
    "plt.title(f'Decoding Latency vs Number of Generated Tokens\\n(Input Length: {input_seq_len}, Averaged over {n_runs} runs)')\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.yscale('log')\n",
    "plt.xscale('log')\n",
    "plt.show()\n",
    "\n",
    "# Print average time per token\n",
    "print(f\"\\nAverage time per decoded token (averaged over {n_runs} runs):\")\n",
    "print(\"----------------------------------------------------\")\n",
    "print(\"Decoding Method | Tokens Decoded | Avg ms/token\")\n",
    "print(\"----------------------------------------------------\")\n",
    "for n_tokens, total_time_no_kv, total_time_kv in zip(num_decode_tokens, decode_times_no_kv, decode_times_kv):\n",
    "    avg_per_token_no_kv = total_time_no_kv / n_tokens if n_tokens > 0 else 0\n",
    "    avg_per_token_kv = total_time_kv / n_tokens if n_tokens > 0 else 0\n",
    "    print(f\"No KV Cache     | {n_tokens:14d} | {avg_per_token_no_kv:12.2f}\")\n",
    "    print(f\"With KV Cache   | {n_tokens:14d} | {avg_per_token_kv:12.2f}\")\n",
    "    print(\"----------------------------------------------------\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                                                                  | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:13<00:00, 13.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens per second with KV cache: 378.62\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "try:\n",
    "    os.chdir(\"/n/home11/tanishqkumar/gravity-chamber/fundamentals/llm\")\n",
    "except FileNotFoundError:\n",
    "    print(\"Warning: Could not change directory. Make sure the path is correct.\")\n",
    "\n",
    "import torch\n",
    "from transformer import Transformer\n",
    "from generation import TransformerGenerator\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "vocab_size = 256\n",
    "hidden_dim = 512\n",
    "n_layers = 6\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# Initialize model with layer_idx for attention layers\n",
    "model = Transformer(depth=n_layers, hidden_dim=hidden_dim, vocab_size=vocab_size, device=device) # , gqa=True\n",
    "model = model.to(device)\n",
    "model.eval()\n",
    "\n",
    "# Initialize generator with model\n",
    "generator = TransformerGenerator(model)\n",
    "\n",
    "input_seq_len = 512\n",
    "num_decode_tokens = [256]\n",
    "n_runs = 20\n",
    "\n",
    "with torch.no_grad():\n",
    "    for n_tokens in tqdm(num_decode_tokens):\n",
    "        batch_size = 1\n",
    "        times_for_this_n_kv = []\n",
    "\n",
    "        for _ in range(n_runs):\n",
    "            # Generate random prompt\n",
    "            prompt_tensor = torch.randint(0, vocab_size, (batch_size, input_seq_len)).to(device)\n",
    "            prompt_list = prompt_tensor.squeeze(0).tolist()\n",
    "\n",
    "            # Time the generation\n",
    "            start_time_kv = time.time()\n",
    "            generated = generator.generate(prompt_list, n_tokens)\n",
    "            torch.cuda.synchronize()\n",
    "            end_time_kv = time.time()\n",
    "            times_for_this_n_kv.append((end_time_kv - start_time_kv) * 1000)\n",
    "\n",
    "\n",
    "        # Calculate and print metrics\n",
    "        avg_time_kv = np.mean(times_for_this_n_kv)\n",
    "        tokens_per_sec = n_tokens / (avg_time_kv / 1000)\n",
    "        print(f\"Tokens per second with KV cache: {tokens_per_sec:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "envi",
   "language": "python",
   "name": "lingua_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
