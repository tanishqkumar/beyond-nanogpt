{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 128, 256])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch \n",
    "import torch.nn as nn \n",
    "import torch.nn.functional as F\n",
    "\n",
    "def apply_rope_embeddings(q, k): # rotate each row in q = [b, s, d] and k\n",
    "    _, s, d = q.shape\n",
    "    device = q.device\n",
    "\n",
    "    # constant from RoPE paper, originall from Transformer paper sinusoidal embeddings\n",
    "    base = torch.tensor(10_000, device=device) \n",
    "    # get positions and freqs, call the joint [s, d] matrix phases\n",
    "    thetas = torch.exp(torch.log(base) * -2.0 * (torch.arange(d//2, device=device))/d)\n",
    "    # this line implicity broadcasts [s] * [d//2] -> [s, d//2] which we'll then alternate to get [s, d]\n",
    "    phases = torch.arange(s, device=device)[:,None] * thetas \n",
    "\n",
    "    # sin/cosify phases \n",
    "    sin, cos = torch.sin(phases).repeat(1, 2), torch.cos(phases).repeat(1, 2)\n",
    "\n",
    "    def _flip(v):\n",
    "        even_v = v[:, 0::2] \n",
    "        neg_odd_v = v[:, 1::2] * -1.\n",
    "\n",
    "        flipped_v = torch.zeros_like(v)\n",
    "        flipped_v[:, 0::2] = neg_odd_v  \n",
    "        flipped_v[:, 1::2] = even_v   \n",
    "        return flipped_v\n",
    "        # flip so that q is now [-q1, q0, -q3, q2...] \n",
    "        # we do this because the multiplication and addition below is mathematically equivalent\n",
    "        # to applying a 2x2 rotation matrix on every pair of entries in each q_i, k_i which are [d]-vectors\n",
    "        # but a sparse matmul would be more expensive compute-wise than just the elementwise ops below\n",
    "\n",
    "    # convolve to simulate sparse matmul and return \n",
    "    q = cos * q + sin * _flip(q)\n",
    "    k = cos * k + sin * _flip(k)\n",
    "\n",
    "    return q, k\n",
    "\n",
    "\n",
    "class MHSA(nn.Module): \n",
    "    def __init__(self, d=512, max_seqlen=1024, b=16): \n",
    "        super().__init__()\n",
    "        self.wq = nn.Linear(d, d)\n",
    "        self.wk = nn.Linear(d, d)\n",
    "        self.wv = nn.Linear(d, d)\n",
    "        self.wo = nn.Linear(d, d)\n",
    "        assert d % 64 == 0 \n",
    "        self.head_dim = 64 \n",
    "        self.nheads = d//self.head_dim\n",
    "\n",
    "    def forward(self, x): # x is [b, s, d]\n",
    "        q, k, v = self.wq(x), self.wk(x), self.wv(x)\n",
    "        b, s, d = q.shape; nh = self.nheads; hd = self.head_dim\n",
    "        # project to heads, each should be [b, nh, s, hd]\n",
    "        \n",
    "        q = q.reshape(b, s, nh, hd).transpose(1, 2) # [b, nh, s, hd]\n",
    "        k = k.reshape(b, s, nh, hd).transpose(1, 2)\n",
    "        v = v.reshape(b, s, nh, hd).transpose(1, 2)\n",
    "\n",
    "        q, k = apply_rope_embeddings(q, k) \n",
    "        # at this point, q and k are cached/cache is updated, ie. after rotation \n",
    "\n",
    "        A_logits = torch.einsum('bnik,bnjk->bnij', q, k) # [b, nh, s, hd] @ [b, nh, s, hd] -> [b, nh, s, s]\n",
    "        A = F.softmax(A_logits/(hd**0.5), dim=-1)\n",
    "\n",
    "        out = torch.einsum('bnij,bnjk->bnik', A, v) # [b, nh, s, s] @ [b, nh, s, hd] -> [b, nh, s, hd]\n",
    "        out = out.transpose(1, 2).reshape(b, s, d)\n",
    "\n",
    "        return self.wo(out)\n",
    "\n",
    "\n",
    "# intuition -> mem/compute (and thus throughput) gains of a factor of (d/c) ~ 10x!\n",
    "# sequence-dependent objectives are isolated from embed_dim, so we see [d] and [S, c] but never [S, d] \n",
    "# like in normal attention\n",
    "\n",
    "# naive question: why can't we just apply_rope_embeddings in the same way as above in MLA? \n",
    "    # why the need for new \"decoupled RoPE\"? \n",
    "    # the reason is because during inference, normally we cache RK ie. the rotated keys \n",
    "    # but with MLA, we aren't storing the keys, and in fact the point is to never \n",
    "    # materialize a [S, d] matrix during inference -- cache is [S, c] << [S, d]\n",
    "    # so we can't just \"expand up and rotate\" since that would defeat the point of never \n",
    "    # materializing [S, d] memory in the first place!\n",
    "\n",
    "    # and even if you did materialize by up projecting (q @ w_d_q) which is a [c]-vector\n",
    "    # to [d] dimensions using w_u_q which is [c, d], note that the rotation matrix R which is [d, d]\n",
    "    # does *not* commute with w_u_q, so it would be wrong (ie. not equivalent to R @ q in regular mhsa)\n",
    "\n",
    "    # so this is why we need \"decoupled rope\" that rotates only the queries (which )\n",
    "\n",
    "# Key intuition: the key fact is that *we never materialize -- either in cache or fwd pass -- an [S, d] key or value matrix\n",
    "# during decoding at all.* We *do attention logit computation \"Q@K.T\" in latent space of dimension c<<d*\n",
    "# ie. down-project q_t to dimension [c] then attend over the cache of dim [c, S] to get an S-vector of logits\n",
    "# so \"latent-attention\" means *attention over projections of keys/values into a small space\"\n",
    "\n",
    "class MLA(nn.Module): \n",
    "    def __init__(self, d=512, c=64, head_dim=64): \n",
    "        super().__init__()\n",
    "        # note that d is the full hidden_dim for the transformer, ie. d = num_heads * head_dim\n",
    "        # and not head dimension, which is, unsurprisingly, head_dim\n",
    "        self.wq = nn.Linear(d, d)\n",
    "        self.wk = nn.Linear(d, d)\n",
    "        self.wv = nn.Linear(d, d)\n",
    "        self.wo = nn.Linear(d, d)\n",
    "        assert d % head_dim == 0 \n",
    "        self.head_dim = head_dim\n",
    "        self.nheads = d//self.head_dim\n",
    "        self.c = c # should have c << d for savings, for us 64 << 512 so approx 10x less KV cache size\n",
    "        \n",
    "        # self.cache would be defined if we had a full inference-ready implementation, it would be [b, max_seqlen, c]\n",
    "        # as opposed to how its usually [b, max_seqlen, d]\n",
    "        # and we'd use it during decoding as \n",
    "        \n",
    "        # then up project at inference time\n",
    "        self.w_d_kv = nn.Linear(d, c)\n",
    "        self.w_d_q = nn.Linear(d, c)\n",
    "\n",
    "        self.w_u_k = nn.Linear(c, self.head_dim * self.nheads)\n",
    "        self.w_u_v = nn.Linear(c, self.head_dim * self.nheads)\n",
    "        self.w_u_q = nn.Linear(c, self.head_dim * self.nheads)\n",
    "\n",
    "        # at inference, we can precompute the below matrices since our weights are frozen \n",
    "        # so that there are no more matmuls then normal attention since up_proj and attn_proj\n",
    "        # get fused into one transformation (ie. what used to be self.wq(x) becomes self.nwq(x))\n",
    "        # if we didn't do this, then projecting c -> d -> d becomes extra compute cost \n",
    "\n",
    "        # self.nwq = self.wq @ self.w_u_q\n",
    "        # self.nwk = self.wq @ self.w_u_q\n",
    "        # self.nwv = self.wq @ self.w_u_q\n",
    "\n",
    "    def forward(self, x): # x is [b, s, d]\n",
    "        c_kv = self.w_d_kv(x) # [b, s, d] @ [d, c] -> [b, s, c]\n",
    "        c_q = self.w_d_q(x) # [b, s, d] @ [d, c] -> [b, s, c]\n",
    "\n",
    "        # if model.eval() and cache is nonempty (ie. prefill is done),\n",
    "        # we would update cache = torch.cat([self.cache, v_kv], dim=-1) at this point\n",
    "        # to grow latent cache during decoding \n",
    "\n",
    "        q, k, v = self.w_u_q(c_q), self.w_u_k(c_kv), self.w_u_v(c_kv) \n",
    "        # each is [b, s, hd * nh] = [b, s, d] where s=1 in decoding\n",
    "        \n",
    "        b, s, d = q.shape; nh = self.nheads; hd = self.head_dim\n",
    "        # project to heads, each should be [b, nh, s, hd]\n",
    "        \n",
    "        q = q.reshape(b, s, nh, hd).transpose(1, 2) # [b, s, d] -> [b, s, nh, hd] -> [b, nh, s, hd]\n",
    "        k = k.reshape(b, s, nh, hd).transpose(1, 2)\n",
    "        v = v.reshape(b, s, nh, hd).transpose(1, 2)\n",
    "\n",
    "        A_logits = torch.einsum('bnik,bnjk->bnij', q, k) # [b, nh, s, hd] @ [b, nh, s, hd] -> [b, nh, s, s]\n",
    "        A = F.softmax(A_logits/(hd**0.5), dim=-1)\n",
    "\n",
    "        out = torch.einsum('bnij,bnjk->bnik', A, v) # [b, nh, s, s] @ [b, nh, s, hd] -> [b, nh, s, hd]\n",
    "        out = out.transpose(1, 2).reshape(b, s, d)\n",
    "\n",
    "        return self.wo(out)\n",
    "\n",
    "\n",
    "b, s, d = 16, 128, 256\n",
    "x = torch.randn(b, s, d)\n",
    "mhsa = MLA(d=d)\n",
    "mhsa(x).shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ MLA causal: full‑sequence == incremental decode\n"
     ]
    }
   ],
   "source": [
    "# mla_causal.py  – Multi‑latent Attention (DeepSeek‑V2, Eq. 9‑13 + causal mask)\n",
    "from __future__ import annotations \n",
    "import math, torch, torch.nn as nn, torch.nn.functional as F\n",
    "from typing import Optional, Tuple\n",
    "\n",
    "# ── helpers ────────────────────────────────────────────────────────────────\n",
    "def precompute_rope(freqs: torch.Tensor, seq_len: int) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "    t = torch.arange(seq_len, device=freqs.device)\n",
    "    freqs = torch.outer(t, freqs)          # [seq, d/2]\n",
    "    return freqs.cos(), freqs.sin()\n",
    "\n",
    "def apply_rope(x: torch.Tensor, cos: torch.Tensor, sin: torch.Tensor) -> torch.Tensor:\n",
    "    x_even, x_odd = x[..., 0::2], x[..., 1::2]\n",
    "    return torch.cat([x_even * cos - x_odd * sin,\n",
    "                      x_even * sin + x_odd * cos], dim=-1)\n",
    "\n",
    "\n",
    "# ── MLA Components ────────────────────────────────────────────────────────\n",
    "class MLAProjections(nn.Module):\n",
    "    \"\"\"Projections for Multi-Latent Attention\"\"\"\n",
    "    def __init__(self, d_model: int, d_c_total: int, n_heads: int, d_r_head: int):\n",
    "        super().__init__()\n",
    "        # latent projections ── Eq.(9‑13)\n",
    "        self.to_q_lat = nn.Linear(d_model, d_c_total, bias=False)   # absorbs W_U^Kᵀ W_U^Q W_D^Q\n",
    "        self.to_c_kv  = nn.Linear(d_model, d_c_total, bias=False)   # W_D^KV\n",
    "        self.to_v     = nn.Linear(d_c_total, d_model, bias=False)   # W_U^V\n",
    "        self.wo       = nn.Linear(d_model, d_model, bias=False)\n",
    "\n",
    "        # positional slice (decoupled RoPE) ── §2.1.3\n",
    "        self.to_q_r = nn.Linear(d_model, n_heads * d_r_head, bias=False)\n",
    "        self.to_k_r = nn.Linear(d_model, d_r_head, bias=False)\n",
    "\n",
    "class RoPEPositionalEncoding(nn.Module):\n",
    "    \"\"\"Rotary Positional Encoding\"\"\"\n",
    "    def __init__(self, d_r_head: int, rope_base: float = 10_000.0):\n",
    "        super().__init__()\n",
    "        inv_freq = 1.0 / (rope_base ** (torch.arange(0, d_r_head, 2).float() / d_r_head))\n",
    "        self.register_buffer(\"inv_freq\", inv_freq, persistent=False)\n",
    "    \n",
    "    def forward(self, seq_len: int, device: torch.device, \n",
    "                start_pos: Optional[int] = None, end_pos: Optional[int] = None) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        cos, sin = precompute_rope(self.inv_freq, seq_len)\n",
    "        if start_pos is not None and end_pos is not None:\n",
    "            pos_idx = torch.arange(start_pos, end_pos, device=device)\n",
    "            return cos[pos_idx], sin[pos_idx]\n",
    "        return cos, sin\n",
    "\n",
    "# ── MLA ────────────────────────────────────────────────────────────────────\n",
    "class MLA(nn.Module):\n",
    "    r\"\"\"Multi‑head Latent Attention (DeepSeek‑V2, §2.1.2–2.1.3) *with causal mask*.\"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 d_model:   int  = 512,\n",
    "                 n_heads:   int  = 8,\n",
    "                 d_c_total: int  = 128,\n",
    "                 d_r_head:  int  = 32,\n",
    "                 rope_base: float = 10_000.0):\n",
    "        super().__init__()\n",
    "        assert d_c_total % n_heads == 0, \"d_c_total must divide n_heads\"\n",
    "        assert d_r_head  % 2       == 0, \"d_r_head must be even\"\n",
    "\n",
    "        # constants\n",
    "        self.h      = n_heads\n",
    "        self.d_c    = d_c_total\n",
    "        self.d_c_h  = d_c_total // n_heads\n",
    "        self.d_r    = d_r_head\n",
    "        self.scale  = 1 / math.sqrt(self.d_c_h + self.d_r)\n",
    "\n",
    "        # components\n",
    "        self.projections = MLAProjections(d_model, d_c_total, n_heads, d_r_head)\n",
    "        self.rope = RoPEPositionalEncoding(d_r_head, rope_base)\n",
    "\n",
    "        self.clear_cache()\n",
    "\n",
    "    # ───────── cache helpers ────────────────────────────\n",
    "    def clear_cache(self):\n",
    "        self.register_buffer(\"_cache_c\", None, persistent=False)  # latent KV   [b,h,L,d_c_h]\n",
    "        self.register_buffer(\"_cache_r\", None, persistent=False)  # rot‑keys    [b,L,d_r]\n",
    "        self._seq_len_cached = 0\n",
    "\n",
    "    # ───────── forward ─────────────────────────────────\n",
    "    def forward(self, x: torch.Tensor, decode: bool = False) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        If `decode=False`  → full‑sequence (training / pre‑fill, causal mask applied).\n",
    "        If `decode=True`   → x is **one token**; uses & extends KV cache.\n",
    "        \"\"\"\n",
    "        b, s, _ = x.shape\n",
    "        device  = x.device\n",
    "\n",
    "        # ── low‑rank projections ─────────────────────────\n",
    "        c_kv  = self.projections.to_c_kv(x)                              # [b,s,d_c]\n",
    "        q_lat = self.projections.to_q_lat(x)                             # [b,s,d_c]\n",
    "\n",
    "        c_kv  = c_kv .view(b, s, self.h, self.d_c_h).transpose(1, 2)  # [b,h,s,d_c_h]\n",
    "        q_lat = q_lat.view(b, s, self.h, self.d_c_h).transpose(1, 2)  # [b,h,s,d_c_h]\n",
    "\n",
    "        # ── positional slice + RoPE ───────────────────────\n",
    "        q_r = self.projections.to_q_r(x).view(b, s, self.h, self.d_r)    # [b,s,h,d_r]\n",
    "        k_r = self.projections.to_k_r(x)                                 # [b,s,d_r]\n",
    "\n",
    "        max_len = self._seq_len_cached + s if decode else s\n",
    "        \n",
    "        if decode:\n",
    "            cos_p, sin_p = self.rope(max_len, device, self._seq_len_cached, self._seq_len_cached + s)\n",
    "        else:\n",
    "            cos, sin = self.rope(max_len, device)\n",
    "            cos_p, sin_p = cos[:s], sin[:s]\n",
    "\n",
    "        cos_q = cos_p[None, :, None, :]                      # [1,s,1,d/2]\n",
    "        sin_q = sin_p[None, :, None, :]\n",
    "        q_r   = apply_rope(q_r, cos_q, sin_q)\n",
    "\n",
    "        cos_k = cos_p[None, :, :]                            # [1,s,d/2]\n",
    "        sin_k = sin_p[None, :, :]\n",
    "        k_r   = apply_rope(k_r, cos_k, sin_k)\n",
    "\n",
    "        # ── update / fetch cache ─────────────────────────\n",
    "        if decode:\n",
    "            self._cache_c = c_kv if self._cache_c is None else torch.cat([self._cache_c, c_kv], dim=2)\n",
    "            self._cache_r = k_r  if self._cache_r is None else torch.cat([self._cache_r, k_r ], dim=1)\n",
    "            self._seq_len_cached += s\n",
    "            k_lat = self._cache_c                              # [b,h,L,d_c_h]\n",
    "            k_r   = self._cache_r                              # [b,L,d_r]\n",
    "        else:\n",
    "            k_lat = c_kv                                       # current sequence\n",
    "            self.clear_cache()                                 # keep cache empty for training\n",
    "\n",
    "        L = k_lat.shape[2]\n",
    "\n",
    "        # ── attention logits  (latent + positional) ─────\n",
    "        content_logits = torch.einsum('bhqd,bhkd->bhqk', q_lat, k_lat)   # [b,h,q,k]\n",
    "        pos_logits     = torch.einsum('bqhd,bkd->bhqk',  q_r,   k_r)     # [b,h,q,k]\n",
    "        logits         = (content_logits + pos_logits) * self.scale\n",
    "\n",
    "        # Apply causal mask when needed\n",
    "        if not decode:\n",
    "            self._apply_causal_mask(logits, s, L, device)\n",
    "\n",
    "        attn = F.softmax(logits, dim=-1)                                   # [b,h,q,k]\n",
    "\n",
    "        # ── values & output ──────────────────────────────\n",
    "        v_all = self._compute_values(k_lat, b, L)\n",
    "        out   = torch.einsum('bhqk,bhkd->bhqd', attn, v_all) \\\n",
    "                    .transpose(1, 2).reshape(b, s, -1)\n",
    "        return self.projections.wo(out)\n",
    "    \n",
    "    def _apply_causal_mask(self, logits: torch.Tensor, s: int, L: int, device: torch.device):\n",
    "        \"\"\"Apply causal mask to attention logits\"\"\"\n",
    "        q_idx = torch.arange(s, device=device)\n",
    "        k_idx = torch.arange(L, device=device)\n",
    "        mask  = (k_idx[None, :] > q_idx[:, None])                      # [s,k]\n",
    "        logits.masked_fill_(mask[None, None, ...], float('-inf'))\n",
    "    \n",
    "    def _compute_values(self, k_lat: torch.Tensor, b: int, L: int) -> torch.Tensor:\n",
    "        \"\"\"Compute values from latent keys\"\"\"\n",
    "        return self.projections.to_v(k_lat.transpose(1, 2).reshape(b, L, self.d_c)) \\\n",
    "                    .view(b, L, self.h, -1).transpose(1, 2)                # [b,h,L,d_h]\n",
    "\n",
    "# ── smoke‑test: causal full pass ≡ streaming decode ─────────────────────────\n",
    "if __name__ == \"__main__\":\n",
    "    torch.manual_seed(0)\n",
    "    mla = MLA(d_model=256, n_heads=4, d_c_total=64, d_r_head=32)\n",
    "    x   = torch.randn(2, 10, 256)\n",
    "\n",
    "    # full‑sequence (masked)\n",
    "    y_full = mla(x)                              # [2,10,256]\n",
    "\n",
    "    # incremental decode\n",
    "    mla.clear_cache()\n",
    "    y_inc = torch.cat([mla(x[:, t:t+1, :], decode=True) for t in range(10)], dim=1)\n",
    "\n",
    "    # they should now be identical\n",
    "    torch.testing.assert_close(y_full, y_inc, atol=1e-5, rtol=0)\n",
    "    print(\"✓ MLA causal: full‑sequence == incremental decode\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ MLA causal: full‑sequence == incremental decode\n"
     ]
    }
   ],
   "source": [
    "## minimal implementation\n",
    "import torch, torch.nn as nn, torch.nn.functional as F\n",
    "import math\n",
    "\n",
    "class MLAProjections(nn.Module): \n",
    "    def __init__(self, d_model: int = 512, d_c_total: int = 64): \n",
    "        super().__init__()\n",
    "\n",
    "        self.to_q_lat = nn.Linear(d_model, d_c_total, bias=False)\n",
    "        self.to_c_kv = nn.Linear(d_model, d_c_total, bias=False)\n",
    "        self.to_v = nn.Linear(d_c_total, d_model, bias=False)\n",
    "        self.wo = nn.Linear(d_model, d_model, bias=False)\n",
    "\n",
    "\n",
    "class MLA(nn.Module): \n",
    "    def __init__(self, d_model: int = 512, d_c_total: int = 64, n_heads: int = 16, causal: bool = True):\n",
    "        super().__init__()\n",
    "\n",
    "        assert d_model % n_heads == 0, \"Error: n_heads must divide d_model for \\\n",
    "            each head to be of the same size!\"\n",
    "        assert d_c_total % n_heads == 0, \"Error: n_heads must divide d_c_total!\"\n",
    "    \n",
    "        self.d_model = d_model \n",
    "        self.d_c_total = d_c_total \n",
    "        self.n_heads = n_heads \n",
    "        self.d_h = int(self.d_model/n_heads)\n",
    "        self.d_c_h = int(d_c_total/n_heads)\n",
    "        self.causal = causal \n",
    "        self.scale = 1.0 / math.sqrt(self.d_c_h)\n",
    "\n",
    "        self.projections = MLAProjections(d_model, d_c_total)\n",
    "\n",
    "        self.clear_cache()\n",
    "\n",
    "    def clear_cache(self):\n",
    "        self.register_buffer(\"c_cache\", None, persistent=False)\n",
    "        self.cache_pos = 0\n",
    "\n",
    "    def _apply_causal_mask(self, logits: torch.Tensor, s: int, L: int, device: torch.device):\n",
    "        \"\"\"Apply causal mask to attention logits\"\"\"\n",
    "        q_idx = torch.arange(s, device=device)\n",
    "        k_idx = torch.arange(L, device=device)\n",
    "        mask = (k_idx[None, :] > q_idx[:, None])                      # [s,k]\n",
    "        logits.masked_fill_(mask[None, None, ...], float('-inf'))\n",
    "\n",
    "    def forward(self, x: torch.Tensor, decode: bool = False): # [b, s, d_model] -> [b, s, d_model]\n",
    "        b, s, d = x.shape \n",
    "        device = x.device \n",
    "\n",
    "        # get qkv, whether through cache or not \n",
    "        q_lat = self.projections.to_q_lat(x) # [b, s, d_c]\n",
    "        c_kv = self.projections.to_c_kv(x)   # [b, s, d_c]\n",
    "\n",
    "        # reshape both [b, s, d_c] ->  [b, nh, s, d_c_h]\n",
    "        q_lat = q_lat.view(b, s, self.n_heads, self.d_c_h).transpose(1, 2) \n",
    "        c_kv = c_kv.view(b, s, self.n_heads, self.d_c_h).transpose(1, 2)\n",
    "\n",
    "        # hit/update cache if decode \n",
    "        if decode: \n",
    "            if self.cache_pos == 0: \n",
    "                self.c_cache = c_kv\n",
    "                self.cache_pos += s                   \n",
    "                k_len = s\n",
    "            else: \n",
    "                c_kv = torch.cat([self.c_cache, c_kv], dim=2) \n",
    "                self.c_cache = c_kv \n",
    "                self.cache_pos += s\n",
    "                k_len = self.c_cache.size(2)\n",
    "        else:\n",
    "            self.clear_cache()\n",
    "            k_len = s\n",
    "\n",
    "        # do attn in latent space \n",
    "        attn_logits = torch.einsum('bhqd,bhkd->bhqk', q_lat, c_kv) * self.scale\n",
    "        \n",
    "        if self.causal and not decode: \n",
    "            self._apply_causal_mask(attn_logits, s, s, device)\n",
    "            \n",
    "        attn = F.softmax(attn_logits, dim=-1)\n",
    "\n",
    "        # project c_kv to values and do A@v \n",
    "        v = self.projections.to_v(c_kv.transpose(1, 2).reshape(b, k_len, self.d_c_total))\n",
    "        v = v.view(b, k_len, self.n_heads, self.d_h).transpose(1, 2)\n",
    "\n",
    "        # return wo(out)\n",
    "        out = torch.einsum('bhqk,bhkd->bhqd', attn, v)\n",
    "        out = out.transpose(1, 2).reshape(b, s, -1)\n",
    "        \n",
    "        return self.projections.wo(out)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    torch.manual_seed(0)\n",
    "    mla = MLA(d_model=256, n_heads=4, d_c_total=64)\n",
    "    x   = torch.randn(2, 10, 256) # b, s, d\n",
    "\n",
    "    # full‑sequence (masked)\n",
    "    y_full = mla(x)                              # [2,10,256]\n",
    "\n",
    "    # incremental decode\n",
    "    mla.clear_cache()\n",
    "    y_inc = torch.cat([mla(x[:, t:t+1, :], decode=True) for t in range(10)], dim=1)\n",
    "\n",
    "    # they should now be identical\n",
    "    torch.testing.assert_close(y_full, y_inc, atol=1e-5, rtol=0)\n",
    "    print(\"✓ MLA causal: full‑sequence == incremental decode\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# understand rope details with math on paper \n",
    "# benchmark against normal mhsa (KV cache size vs seqlen, throughput, tok/s)\n",
    "# grok why incremental vs one-shot should result in same state/kvc "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "envi",
   "language": "python",
   "name": "lingua_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
