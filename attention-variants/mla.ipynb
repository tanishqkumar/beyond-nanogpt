{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 128, 256])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch \n",
    "import torch.nn as nn \n",
    "import torch.nn.functional as F\n",
    "\n",
    "def apply_rope_embeddings(q, k): # rotate each row in q = [b, s, d] and k\n",
    "    _, s, d = q.shape\n",
    "    device = q.device\n",
    "\n",
    "    # constant from RoPE paper, originall from Transformer paper sinusoidal embeddings\n",
    "    base = torch.tensor(10_000, device=device) \n",
    "    # get positions and freqs, call the joint [s, d] matrix phases\n",
    "    thetas = torch.exp(torch.log(base) * -2.0 * (torch.arange(d//2, device=device))/d)\n",
    "    # this line implicity broadcasts [s] * [d//2] -> [s, d//2] which we'll then alternate to get [s, d]\n",
    "    phases = torch.arange(s, device=device)[:,None] * thetas \n",
    "\n",
    "    # sin/cosify phases \n",
    "    sin, cos = torch.sin(phases).repeat(1, 2), torch.cos(phases).repeat(1, 2)\n",
    "\n",
    "    def _flip(v):\n",
    "        even_v = v[:, 0::2] \n",
    "        neg_odd_v = v[:, 1::2] * -1.\n",
    "\n",
    "        flipped_v = torch.zeros_like(v)\n",
    "        flipped_v[:, 0::2] = neg_odd_v  \n",
    "        flipped_v[:, 1::2] = even_v   \n",
    "        return flipped_v\n",
    "        # flip so that q is now [-q1, q0, -q3, q2...] \n",
    "        # we do this because the multiplication and addition below is mathematically equivalent\n",
    "        # to applying a 2x2 rotation matrix on every pair of entries in each q_i, k_i which are [d]-vectors\n",
    "        # but a sparse matmul would be more expensive compute-wise than just the elementwise ops below\n",
    "\n",
    "    # convolve to simulate sparse matmul and return \n",
    "    q = cos * q + sin * _flip(q)\n",
    "    k = cos * k + sin * _flip(k)\n",
    "\n",
    "    return q, k\n",
    "\n",
    "\n",
    "class MHSA(nn.Module): \n",
    "    def __init__(self, d=512, max_seqlen=1024, b=16): \n",
    "        super().__init__()\n",
    "        self.wq = nn.Linear(d, d)\n",
    "        self.wk = nn.Linear(d, d)\n",
    "        self.wv = nn.Linear(d, d)\n",
    "        self.wo = nn.Linear(d, d)\n",
    "        assert d % 64 == 0 \n",
    "        self.head_dim = 64 \n",
    "        self.nheads = d//self.head_dim\n",
    "\n",
    "    def forward(self, x): # x is [b, s, d]\n",
    "        q, k, v = self.wq(x), self.wk(x), self.wv(x)\n",
    "        b, s, d = q.shape; nh = self.nheads; hd = self.head_dim\n",
    "        # project to heads, each should be [b, nh, s, hd]\n",
    "        \n",
    "        q = q.reshape(b, s, nh, hd).transpose(1, 2) # [b, nh, s, hd]\n",
    "        k = k.reshape(b, s, nh, hd).transpose(1, 2)\n",
    "        v = v.reshape(b, s, nh, hd).transpose(1, 2)\n",
    "\n",
    "        q, k = apply_rope_embeddings(q, k) \n",
    "        # at this point, q and k are cached/cache is updated, ie. after rotation \n",
    "\n",
    "        A_logits = torch.einsum('bnik,bnjk->bnij', q, k) # [b, nh, s, hd] @ [b, nh, s, hd] -> [b, nh, s, s]\n",
    "        A = F.softmax(A_logits/(hd**0.5), dim=-1)\n",
    "\n",
    "        out = torch.einsum('bnij,bnjk->bnik', A, v) # [b, nh, s, s] @ [b, nh, s, hd] -> [b, nh, s, hd]\n",
    "        out = out.transpose(1, 2).reshape(b, s, d)\n",
    "\n",
    "        return self.wo(out)\n",
    "\n",
    "\n",
    "# intuition -> mem/compute (and thus throughput) gains of a factor of (d/c) ~ 10x!\n",
    "# sequence-dependent objectives are isolated from embed_dim, so we see [d] and [S, c] but never [S, d] \n",
    "# like in normal attention\n",
    "\n",
    "# naive question: why can't we just apply_rope_embeddings in the same way as above in MLA? \n",
    "    # why the need for new \"decoupled RoPE\"? \n",
    "    # the reason is because during inference, normally we cache RK ie. the rotated keys \n",
    "    # but with MLA, we aren't storing the keys, and in fact the point is to never \n",
    "    # materialize a [S, d] matrix during inference -- cache is [S, c] << [S, d]\n",
    "    # so we can't just \"expand up and rotate\" since that would defeat the point of never \n",
    "    # materializing [S, d] memory in the first place!\n",
    "\n",
    "    # and even if you did materialize by up projecting (q @ w_d_q) which is a [c]-vector\n",
    "    # to [d] dimensions using w_u_q which is [c, d], note that the rotation matrix R which is [d, d]\n",
    "    # does *not* commute with w_u_q, so it would be wrong (ie. not equivalent to R @ q in regular mhsa)\n",
    "\n",
    "    # so this is why we need \"decoupled rope\" that rotates only the queries (which )\n",
    "\n",
    "# Key intuition: the key fact is that *we never materialize -- either in cache or fwd pass -- an [S, d] key or value matrix\n",
    "# during decoding at all.* We *do attention logit computation \"Q@K.T\" in latent space of dimension c<<d*\n",
    "# ie. down-project q_t to dimension [c] then attend over the cache of dim [c, S] to get an S-vector of logits\n",
    "# so \"latent-attention\" means *attention over projections of keys/values into a small space\"\n",
    "\n",
    "class MLA(nn.Module): \n",
    "    def __init__(self, d=512, c=64, head_dim=64): \n",
    "        super().__init__()\n",
    "        # note that d is the full hidden_dim for the transformer, ie. d = num_heads * head_dim\n",
    "        # and not head dimension, which is, unsurprisingly, head_dim\n",
    "        self.wq = nn.Linear(d, d)\n",
    "        self.wk = nn.Linear(d, d)\n",
    "        self.wv = nn.Linear(d, d)\n",
    "        self.wo = nn.Linear(d, d)\n",
    "        assert d % head_dim == 0 \n",
    "        self.head_dim = head_dim\n",
    "        self.nheads = d//self.head_dim\n",
    "        self.c = c # should have c << d for savings, for us 64 << 512 so approx 10x less KV cache size\n",
    "        \n",
    "        # self.cache would be defined if we had a full inference-ready implementation, it would be [b, max_seqlen, c]\n",
    "        # as opposed to how its usually [b, max_seqlen, d]\n",
    "        # and we'd use it during decoding as \n",
    "        \n",
    "        # then up project at inference time\n",
    "        self.w_d_kv = nn.Linear(d, c)\n",
    "        self.w_d_q = nn.Linear(d, c)\n",
    "\n",
    "        self.w_u_k = nn.Linear(c, self.head_dim * self.nheads)\n",
    "        self.w_u_v = nn.Linear(c, self.head_dim * self.nheads)\n",
    "        self.w_u_q = nn.Linear(c, self.head_dim * self.nheads)\n",
    "\n",
    "        # at inference, we can precompute the below matrices since our weights are frozen \n",
    "        # so that there are no more matmuls then normal attention since up_proj and attn_proj\n",
    "        # get fused into one transformation (ie. what used to be self.wq(x) becomes self.nwq(x))\n",
    "        # if we didn't do this, then projecting c -> d -> d becomes extra compute cost \n",
    "\n",
    "        # self.nwq = self.wq @ self.w_u_q\n",
    "        # self.nwk = self.wq @ self.w_u_q\n",
    "        # self.nwv = self.wq @ self.w_u_q\n",
    "\n",
    "    def forward(self, x): # x is [b, s, d]\n",
    "        c_kv = self.w_d_kv(x) # [b, s, d] @ [d, c] -> [b, s, c]\n",
    "        c_q = self.w_d_q(x) # [b, s, d] @ [d, c] -> [b, s, c]\n",
    "\n",
    "        # if model.eval() and cache is nonempty (ie. prefill is done),\n",
    "        # we would update cache = torch.cat([self.cache, v_kv], dim=-1) at this point\n",
    "        # to grow latent cache during decoding \n",
    "\n",
    "        q, k, v = self.w_u_q(c_q), self.w_u_k(c_kv), self.w_u_v(c_kv) \n",
    "        # each is [b, s, hd * nh] = [b, s, d] where s=1 in decoding\n",
    "        \n",
    "        b, s, d = q.shape; nh = self.nheads; hd = self.head_dim\n",
    "        # project to heads, each should be [b, nh, s, hd]\n",
    "        \n",
    "        q = q.reshape(b, s, nh, hd).transpose(1, 2) # [b, s, d] -> [b, s, nh, hd] -> [b, nh, s, hd]\n",
    "        k = k.reshape(b, s, nh, hd).transpose(1, 2)\n",
    "        v = v.reshape(b, s, nh, hd).transpose(1, 2)\n",
    "\n",
    "        A_logits = torch.einsum('bnik,bnjk->bnij', q, k) # [b, nh, s, hd] @ [b, nh, s, hd] -> [b, nh, s, s]\n",
    "        A = F.softmax(A_logits/(hd**0.5), dim=-1)\n",
    "\n",
    "        out = torch.einsum('bnij,bnjk->bnik', A, v) # [b, nh, s, s] @ [b, nh, s, hd] -> [b, nh, s, hd]\n",
    "        out = out.transpose(1, 2).reshape(b, s, d)\n",
    "\n",
    "        return self.wo(out)\n",
    "\n",
    "\n",
    "b, s, d = 16, 128, 256\n",
    "x = torch.randn(b, s, d)\n",
    "mhsa = MLA(d=d)\n",
    "mhsa(x).shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, torch.nn as nn, torch.nn.functional as F\n",
    "\n",
    "# ---------- helper: 2×2 block rotation (RoPE) ----------\n",
    "def rope_rotate(x):\n",
    "    \"\"\"\n",
    "    x: [..., L] where L is even and represents (cos, sin) pairs\n",
    "    returns rotated tensor of the same shape\n",
    "    \"\"\"\n",
    "    cos, sin = x[..., 0::2], x[..., 1::2]\n",
    "    x1 = torch.stack([-sin, cos], dim=-1).reshape_as(x)\n",
    "    return x1\n",
    "\n",
    "def apply_rope(x, pos):\n",
    "    \"\"\"\n",
    "    x : [B, T, H, d_r]     (d_r even)\n",
    "    pos: [T] or [B, T]\n",
    "    Rotates the last dim in‑place.\n",
    "    \"\"\"\n",
    "    B, T, H, d_r = x.shape\n",
    "    device = x.device\n",
    "    base = 10_000.0\n",
    "\n",
    "    idx = torch.arange(d_r // 2, device=device)\n",
    "    theta = base ** (-2.0 * idx / d_r)                         # [d_r/2]\n",
    "    phase = pos.to(device).unsqueeze(-1) * theta               # [B?,T,d_r/2]\n",
    "    cos, sin = torch.cos(phase), torch.sin(phase)\n",
    "    rot = torch.stack([cos, sin], dim=-1).repeat_interleave(2, dim=-1)\n",
    "    x_rot = rot * x + rope_rotate(rot * x)      # element‑wise\n",
    "    return x_rot\n",
    "# ---------------------------------------------------------\n",
    "\n",
    "\n",
    "class MLA(nn.Module):\n",
    "    \"\"\"\n",
    "    Multi‑head Latent Attention with **decoupled RoPE**.\n",
    "\n",
    "    Args\n",
    "    ----\n",
    "    d_model   : full hidden width (d)\n",
    "    head_dim  : per‑head width (d_h)\n",
    "    d_c       : latent width  (c  << d)\n",
    "    d_r       : width of the RoPE channel per head (even, ≤ d_h)\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model: int, head_dim: int = 64,\n",
    "                 d_c: int = 512, d_r: int = 32):\n",
    "        super().__init__()\n",
    "        assert d_model % head_dim == 0, \"d_model must be H * head_dim\"\n",
    "        assert d_r % 2 == 0 and d_r <= head_dim\n",
    "\n",
    "        self.d, self.hd, self.H, self.c, self.dr = (\n",
    "            d_model, head_dim, d_model // head_dim, d_c, d_r\n",
    "        )\n",
    "        d_content = head_dim - d_r                   # un‑rotated slice per head\n",
    "        self.dc_r = d_r * self.H                     # total rotated width\n",
    "        self.dc_c = d_content * self.H               # total content width\n",
    "\n",
    "        # --------- low‑rank projections ---------\n",
    "        self.W_D_kv = nn.Linear(self.d, self.c, bias=False)\n",
    "        self.W_D_q  = nn.Linear(self.d, self.c, bias=False)\n",
    "\n",
    "        # up‑proj split into \"rot\" and \"content\" parts\n",
    "        self.W_U_k_rot = nn.Linear(self.c, self.dc_r, bias=False)\n",
    "        self.W_U_k_con = nn.Linear(self.c, self.dc_c, bias=False)\n",
    "        self.W_U_v     = nn.Linear(self.c, self.d,    bias=False)\n",
    "\n",
    "        self.W_U_q_rot = nn.Linear(self.c, self.dc_r, bias=False)\n",
    "        self.W_U_q_con = nn.Linear(self.c, self.dc_c, bias=False)\n",
    "\n",
    "        self.out_proj  = nn.Linear(self.d, self.d, bias=False)\n",
    "\n",
    "        # ------------- cache -------------\n",
    "        self.register_buffer(\"cache_latent\", None, persistent=False)\n",
    "        self.register_buffer(\"cache_pos\",    None, persistent=False)\n",
    "\n",
    "    # -------- attention kernel in latent space --------\n",
    "    def _latent_attention(self, q_lat, C_lat):\n",
    "        \"\"\"\n",
    "        q_lat : [B, H, c]      current query already premultiplied by W_U_k*\n",
    "        C_lat : [S, c]         latent cache of keys (shared across heads)\n",
    "        returns context_lat : [B, H, c]\n",
    "        \"\"\"\n",
    "        # scores  [B,H,S]\n",
    "        scores = torch.matmul(q_lat, C_lat.t())          # [B,H,S]\n",
    "        attn   = F.softmax(scores / (self.c ** 0.5), dim=-1)\n",
    "\n",
    "        # context in latent       [B,H,c]\n",
    "        ctx_lat = torch.matmul(attn, C_lat)              # [B,H,c]\n",
    "        return ctx_lat\n",
    "\n",
    "    # ------------- main forward -------------\n",
    "    def forward(self, x, *, decode=False):\n",
    "        \"\"\"\n",
    "        x      : [B,T,d]  (T may be 1 during decoding)\n",
    "        decode : if True → one‑token step with latent cache\n",
    "        returns y : [B,T,d]\n",
    "        \"\"\"\n",
    "        B, T, _ = x.shape\n",
    "        device  = x.device\n",
    "\n",
    "        # ---- 1. down‑project ----\n",
    "        c_kv = self.W_D_kv(x)     # [B,T,c]\n",
    "        c_q  = self.W_D_q (x)     # [B,T,c]\n",
    "\n",
    "        # ---- 2. split up‑projection (rot vs content) ----\n",
    "        q_rot = self.W_U_q_rot(c_q)   # [B,T,dc_r]\n",
    "        q_con = self.W_U_q_con(c_q)   # [B,T,dc_c]\n",
    "\n",
    "        k_rot = self.W_U_k_rot(c_kv)  # [B,T,dc_r]\n",
    "        k_con = self.W_U_k_con(c_kv)  # [B,T,dc_c]\n",
    "        v_all = self.W_U_v    (c_kv)  # [B,T,d]\n",
    "\n",
    "        # ---- 3. reshape to heads ----\n",
    "        def split_heads(t, per_head):\n",
    "            return t.view(B, T, self.H, per_head).transpose(1, 2)  # [B,H,T,per_head]\n",
    "\n",
    "        q_rot = split_heads(q_rot, self.dr)\n",
    "        k_rot = split_heads(k_rot, self.dr)\n",
    "        q_con = split_heads(q_con, self.hd - self.dr)\n",
    "        k_con = split_heads(k_con, self.hd - self.dr)\n",
    "        v     = split_heads(v_all, self.hd)\n",
    "\n",
    "        # ---- 4. apply RoPE only on the small channel ----\n",
    "        pos = (torch.arange(T, device=device)\n",
    "               if not decode else self.cache_pos[-1:] + 1)   # simple counter\n",
    "        q_rot = apply_rope(q_rot, pos)   # each: [B,H,T,dr]\n",
    "        k_rot = apply_rope(k_rot, pos)\n",
    "\n",
    "        # ---- 5. concatenate rotated + content ----\n",
    "        q = torch.cat([q_rot, q_con], dim=-1)   # [B,H,T,d_h]\n",
    "        k = torch.cat([k_rot, k_con], dim=-1)\n",
    "\n",
    "        # ------------------------------------------------------------------\n",
    "        if not decode:                       # ---------- TRAIN MODE ----------\n",
    "            # materialise full K,V for efficiency\n",
    "            scores = torch.einsum('bhid,bhjd->bhij', q, k)        # [B,H,T,T]\n",
    "            attn   = F.softmax(scores / (self.hd ** 0.5), dim=-1)\n",
    "            ctx    = torch.einsum('bhij,bhjd->bhid', attn, v)     # [B,H,T,d_h]\n",
    "            y = ctx.transpose(1, 2).reshape(B, T, self.d)\n",
    "            return self.out_proj(y)\n",
    "\n",
    "        # ------------------------------------------------------------------\n",
    "        else:                                 # ---------- DECODE MODE ----------\n",
    "            assert T == 1, \"decode expects a single new token\"\n",
    "            # initialise cache buffers if empty\n",
    "            if self.cache_latent is None:\n",
    "                self.cache_latent = c_kv[:, -1].detach().clone()  # [1,c]\n",
    "                self.cache_pos    = torch.tensor([-1], device=device, dtype=torch.long)\n",
    "            # append new latent & pos\n",
    "            self.cache_latent = torch.cat([self.cache_latent, c_kv[:, -1]], dim=0)    # [S,c]\n",
    "            self.cache_pos    = torch.cat([self.cache_pos,    pos],         dim=0)    # [S]\n",
    "\n",
    "            # ---- operate purely in latent space ----\n",
    "            Wk = torch.cat([self.W_U_k_rot.weight,  # [dc_r,c]\n",
    "                            self.W_U_k_con.weight], dim=0)        # [d_h ,c]\n",
    "\n",
    "            # pre‑multiply query once per head\n",
    "            q_lat = torch.einsum('bhid,dc->bhc', q, Wk)           # [B,H,c]\n",
    "\n",
    "            ctx_lat = self._latent_attention(q_lat, self.cache_latent)     # [B,H,c]\n",
    "            # up‑project to d\n",
    "            y = torch.einsum('bhc,cd->bhd', ctx_lat,\n",
    "                             torch.cat([self.W_U_v.weight], dim=0))        # [B,H,d_h]\n",
    "            y = y.transpose(1, 2).reshape(B, 1, self.d)\n",
    "            return self.out_proj(y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (4) must match the size of tensor b (32) at non-singleton dimension 3",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# TRAIN / PREFILL\u001b[39;00m\n\u001b[1;32m      5\u001b[0m mla \u001b[38;5;241m=\u001b[39m MLA(d_model\u001b[38;5;241m=\u001b[39md, head_dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m64\u001b[39m, d_c\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m512\u001b[39m, d_r\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m32\u001b[39m)\n\u001b[0;32m----> 6\u001b[0m y \u001b[38;5;241m=\u001b[39m mla(x, decode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)        \u001b[38;5;66;03m# full‑sequence forward\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# AUTOREGRESSIVE DECODE (one token at a time)\u001b[39;00m\n\u001b[1;32m      9\u001b[0m mla\u001b[38;5;241m.\u001b[39meval()\n",
      "File \u001b[0;32m~/.conda/envs/lingua_env/lib/python3.11/site-packages/torch/nn/modules/module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/.conda/envs/lingua_env/lib/python3.11/site-packages/torch/nn/modules/module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[0;32mIn[13], line 125\u001b[0m, in \u001b[0;36mMLA.forward\u001b[0;34m(self, x, decode)\u001b[0m\n\u001b[1;32m    122\u001b[0m \u001b[38;5;66;03m# ---- 4. apply RoPE only on the small channel ----\u001b[39;00m\n\u001b[1;32m    123\u001b[0m pos \u001b[38;5;241m=\u001b[39m (torch\u001b[38;5;241m.\u001b[39marange(T, device\u001b[38;5;241m=\u001b[39mdevice)\n\u001b[1;32m    124\u001b[0m        \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m decode \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcache_pos[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m:] \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m)   \u001b[38;5;66;03m# simple counter\u001b[39;00m\n\u001b[0;32m--> 125\u001b[0m q_rot \u001b[38;5;241m=\u001b[39m apply_rope(q_rot, pos)   \u001b[38;5;66;03m# each: [B,H,T,dr]\u001b[39;00m\n\u001b[1;32m    126\u001b[0m k_rot \u001b[38;5;241m=\u001b[39m apply_rope(k_rot, pos)\n\u001b[1;32m    128\u001b[0m \u001b[38;5;66;03m# ---- 5. concatenate rotated + content ----\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[13], line 28\u001b[0m, in \u001b[0;36mapply_rope\u001b[0;34m(x, pos)\u001b[0m\n\u001b[1;32m     26\u001b[0m cos, sin \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcos(phase), torch\u001b[38;5;241m.\u001b[39msin(phase)\n\u001b[1;32m     27\u001b[0m rot \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mstack([cos, sin], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mrepeat_interleave(\u001b[38;5;241m2\u001b[39m, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m---> 28\u001b[0m x_rot \u001b[38;5;241m=\u001b[39m rot \u001b[38;5;241m*\u001b[39m x \u001b[38;5;241m+\u001b[39m rope_rotate(rot \u001b[38;5;241m*\u001b[39m x)      \u001b[38;5;66;03m# element‑wise\u001b[39;00m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m x_rot\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (4) must match the size of tensor b (32) at non-singleton dimension 3"
     ]
    }
   ],
   "source": [
    "B, S, d = 16, 128, 512\n",
    "x = torch.randn(B, S, d)\n",
    "\n",
    "# TRAIN / PREFILL\n",
    "mla = MLA(d_model=d, head_dim=64, d_c=512, d_r=32)\n",
    "y = mla(x, decode=False)        # full‑sequence forward\n",
    "\n",
    "# AUTOREGRESSIVE DECODE (one token at a time)\n",
    "mla.eval()\n",
    "token = torch.randn(1, 1, d)\n",
    "out1  = mla(token, decode=True)  # step 1\n",
    "token = torch.randn(1, 1, d)\n",
    "out2  = mla(token, decode=True)  # step 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "envi",
   "language": "python",
   "name": "lingua_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
