{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "class MHSA(nn.Module): \n",
    "    def __init__(self, D, head_dim=64):\n",
    "        super().__init__()\n",
    "        self.D = D\n",
    "        self.wq = nn.Linear(D,D)\n",
    "        self.wk = nn.Linear(D,D)\n",
    "        self.wv = nn.Linear(D,D)\n",
    "        self.wo = nn.Linear(D,D)\n",
    "        \n",
    "        assert D % head_dim == 0 \n",
    "        self.n_heads = D//head_dim \n",
    "        self.head_dim = head_dim \n",
    "\n",
    "    def forward(self, x): # BSD -> BSD \n",
    "        B, S, D = x.shape\n",
    "        q, k, v = self.wq(x), self.wk(x), self.wv(x) # BSD -> BSD\n",
    "\n",
    "        # view as B, n_heads, S, head_dim\n",
    "        q = q.view(B, self.n_heads, S, self.head_dim)\n",
    "        k = k.view(B, self.n_heads, S, self.head_dim)\n",
    "        v = v.view(B, self.n_heads, S, self.head_dim)\n",
    "        \n",
    "        # compute attn scores using einsum to do q@k.t within each head now \n",
    "        scores = torch.einsum('bnqd,bnkd->bnqk', q, k) # [batch, nheads, seq, seq]\n",
    "        scores = scores / (self.head_dim ** 0.5) # scale by sqrt(d_k)\n",
    "        \n",
    "        # the dim=-1 below was always confusing to me when learning about softmax\n",
    "        # since we want to normalize each row \n",
    "        # the intuition is that to normalize each row, we want the COLUMNS to sum to 1\n",
    "        # hence should tell torch to softmax over the cols, which here is the last dim of A \n",
    "        # since A is [B, S, S] \n",
    "        \n",
    "        A = F.softmax(scores, dim=-1)\n",
    "        \n",
    "        # apply attention: multiply attention weights with values\n",
    "        # A is [batch, n_heads, seq, seq]\n",
    "        # v is [batch, n_heads, seq, head_dim]\n",
    "        # we want out to be [batch, n_heads, seq, head_dim]\n",
    "        out = torch.einsum('bnqk,bnkd->bnqd', A, v)\n",
    "\n",
    "        # A is BSS and v is BSD\n",
    "        # A@v is BSD \n",
    "        # A is queries (rows) by keys (cols), so want all rows to sum to 1\n",
    "        # because a query can only pay unit attention over all keys behind it\n",
    "        \n",
    "        # reshape from [batch, n_heads, seq, head_dim] back to [batch, seq, dim]\n",
    "        out = out.view(B, S, D)\n",
    "        \n",
    "        return self.wo(out)\n",
    "\n",
    "# test it\n",
    "if __name__ == \"__main__\":\n",
    "    # Set random seed for reproducibility\n",
    "    torch.manual_seed(42)\n",
    "    \n",
    "    # Test parameters\n",
    "    batch_size = 2\n",
    "    seq_len = 4\n",
    "    dim = 128\n",
    "    head_dim = 32\n",
    "    \n",
    "    # Create random input tensor\n",
    "    x = torch.randn(batch_size, seq_len, dim)\n",
    "    \n",
    "    # Initialize MHSA module\n",
    "    mhsa = MHSA(D=dim, head_dim=head_dim)\n",
    "    \n",
    "    # Forward pass\n",
    "    output = mhsa(x)\n",
    "    \n",
    "    # Basic shape tests\n",
    "    assert output.shape == (batch_size, seq_len, dim), f\"Expected shape {(batch_size, seq_len, dim)} but got {output.shape}\"\n",
    "    \n",
    "    # Test attention weights sum to 1\n",
    "    q, k, v = mhsa.wq(x), mhsa.wk(x), mhsa.wv(x)\n",
    "    q = q.view(batch_size, mhsa.n_heads, seq_len, head_dim)\n",
    "    k = k.view(batch_size, mhsa.n_heads, seq_len, head_dim)\n",
    "    scores = torch.einsum('bnqd,bnkd->bnqk', q, k) / (head_dim ** 0.5)\n",
    "    attn_weights = F.softmax(scores, dim=-1)\n",
    "    \n",
    "    # Check if attention weights sum to 1 (with small numerical tolerance)\n",
    "    assert torch.allclose(attn_weights.sum(dim=-1), torch.ones_like(attn_weights.sum(dim=-1)), atol=1e-6)\n",
    "    \n",
    "    print(\"All tests passed!\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
