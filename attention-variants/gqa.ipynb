{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import torch \n",
    "\n",
    "# mhsa baseline to compare gqa  to \n",
    "class mhsa(torch.nn.Module): \n",
    "    def __init__(self, D=512, head_dim=64, causal=True): \n",
    "        super().__init__()\n",
    "        self.D = D \n",
    "        self.head_dim = head_dim \n",
    "        assert self.D % self.head_dim == 0 \n",
    "        self.nheads = self.D // self.head_dim \n",
    "        self.causal = causal \n",
    "\n",
    "        self.wq = torch.nn.Linear(D, D)\n",
    "        self.wk = torch.nn.Linear(D, D)\n",
    "        self.wv = torch.nn.Linear(D, D)\n",
    "        self.wo = torch.nn.Linear(D, D)\n",
    "\n",
    "\n",
    "    def forward(self, x): # BSD -> BSD\n",
    "        B, S, D = x.shape\n",
    "        q, k, v = self.wq(x), self.wk(x), self.wv(x) # x is BSD wq is DD\n",
    "\n",
    "        q = q.reshape(B, self.nheads, S, self.head_dim)\n",
    "        k = k.reshape(B, self.nheads, S, self.head_dim)\n",
    "        v = v.reshape(B, self.nheads, S, self.head_dim) \n",
    "\n",
    "        normalize = torch.sqrt(torch.tensor(self.head_dim))\n",
    "        A = torch.einsum('bnij,bnkj->bnik', q, k) # [B, N, S, D] @ [B, N, S, D] -> [B, N, S, S]\n",
    "        A = torch.nn.functional.softmax(A/normalize, dim=-1)\n",
    "\n",
    "        # check if causal mask \n",
    "        if self.causal: # add -inf in A[j>i]\n",
    "            mask = torch.triu(torch.ones_like(A), diagonal=1).bool()\n",
    "            A = A.masked_fill(mask, float('-inf'))\n",
    "\n",
    "        preout = torch.einsum('bnij,bnjd->bnid', A, v) # BNSS @ BNSD -> BNSD\n",
    "        preout = preout.reshape(B, S, -1) # this concats under the hood \n",
    "        return self.wo(preout)\n",
    "\n",
    "B, S, D = 8, 512, 768\n",
    "attn = mhsa(D=D) \n",
    "x = torch.randn(B, S, D)\n",
    "attn(x).shape # BSD -> BSD \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "\n",
    "class gqa(torch.nn.Module): \n",
    "    def __init__(self, D=512, head_dim=64, causal=True, group_size=4): \n",
    "        super().__init__()\n",
    "        self.D = D \n",
    "        self.head_dim = head_dim \n",
    "        assert self.D % self.head_dim == 0 \n",
    "        self.num_query_heads = self.D // self.head_dim \n",
    "        self.group_size = group_size\n",
    "        # In GQA, keys and values use fewer heads: \n",
    "        # each group of query heads will share a common key/value, so:\n",
    "        self.num_kv_heads = self.num_query_heads // self.group_size \n",
    "        self.causal = causal\n",
    "\n",
    "        self.D_kv = self.num_kv_heads * self.head_dim\n",
    "\n",
    "        self.wq = torch.nn.Linear(D, D)\n",
    "        self.wk = torch.nn.Linear(D, self.D_kv)\n",
    "        self.wv = torch.nn.Linear(D, self.D_kv)\n",
    "        self.wo = torch.nn.Linear(D, D)\n",
    "\n",
    "\n",
    "    def forward(self, x):  # Input x: [B, S, D] -> Output: [B, S, D]\n",
    "        B, S, D = x.shape\n",
    "        q = self.wq(x)  # [B, S, D]\n",
    "        k = self.wk(x)  # [B, S, D_kv]\n",
    "        v = self.wv(x)  # [B, S, D_kv]\n",
    "\n",
    "        # Reshape queries to have all query heads.\n",
    "        q = q.reshape(B, self.num_query_heads, S, self.head_dim)\n",
    "        k = k.reshape(B, self.num_kv_heads, S, self.head_dim)\n",
    "        v = v.reshape(B, self.num_kv_heads, S, self.head_dim)\n",
    "\n",
    "        # reshape kv to match q by interleaving\n",
    "        k = torch.repeat_interleave(k, self.group_size, dim=1)\n",
    "        v = torch.repeat_interleave(v, self.group_size, dim=1)\n",
    "\n",
    "        # Compute scaled dot-product attention.\n",
    "        normalize = torch.sqrt(torch.tensor(self.head_dim, dtype=q.dtype, device=q.device))\n",
    "        logits = torch.einsum('bnij,bnkj->bnik', q, k) / normalize  # [B, num_query_heads, S, S]\n",
    "\n",
    "        # Apply causal mask if needed.\n",
    "        if self.causal:\n",
    "            mask = torch.triu(torch.ones_like(logits), diagonal=1).bool()\n",
    "            logits = logits.masked_fill(mask, float('-inf'))\n",
    "\n",
    "        A = torch.nn.functional.softmax(logits, dim=-1)\n",
    "        preout = torch.einsum('bnij,bnjd->bnid', A, v)  # [B, num_query_heads, S, head_dim]\n",
    "        preout = preout.reshape(B, S, -1)  # Concatenate heads: [B, S, D]\n",
    "        return self.wo(preout)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "\n",
    "# Test different sequence lengths\n",
    "seq_lengths = [128, 256, 512, 1024, 2048, 4096, 8192]\n",
    "B, D = 1, 1024  # Keep batch size and dimension fixed\n",
    "\n",
    "gqa_times = []\n",
    "mhsa_times = []\n",
    "\n",
    "for S in seq_lengths:\n",
    "    # Create inputs\n",
    "    x = torch.randn(B, S, D, device='cuda')\n",
    "    \n",
    "    # Test GQA\n",
    "    attn_gqa = gqa(D=D).cuda()\n",
    "    \n",
    "    # Warmup\n",
    "    for _ in range(5):\n",
    "        _ = attn_gqa(x)\n",
    "    \n",
    "    # Measure GQA time\n",
    "    start = time.perf_counter()\n",
    "    for _ in range(10):\n",
    "        _ = attn_gqa(x)\n",
    "    torch.cuda.synchronize()\n",
    "    gqa_times.append((time.perf_counter() - start) / 10)\n",
    "\n",
    "    # Test MHSA \n",
    "    attn_mhsa = mhsa(D=D).cuda()\n",
    "    \n",
    "    # Warmup\n",
    "    for _ in range(5):\n",
    "        _ = attn_mhsa(x)\n",
    "    \n",
    "    # Measure MHSA time\n",
    "    start = time.perf_counter()\n",
    "    for _ in range(10):\n",
    "        _ = attn_mhsa(x)\n",
    "    torch.cuda.synchronize()\n",
    "    mhsa_times.append((time.perf_counter() - start) / 10)\n",
    "\n",
    "# Plot results\n",
    "x = np.arange(len(seq_lengths))\n",
    "width = 0.35\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "ax.bar(x - width/2, gqa_times, width, label='GQA')\n",
    "ax.bar(x + width/2, mhsa_times, width, label='MHSA')\n",
    "\n",
    "ax.set_ylabel('Latency (seconds)')\n",
    "ax.set_xlabel('Sequence Length')\n",
    "ax.set_title('GQA vs MHSA Latency Comparison')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(seq_lengths)\n",
    "ax.legend()\n",
    "\n",
    "# plt.yscale('log')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
