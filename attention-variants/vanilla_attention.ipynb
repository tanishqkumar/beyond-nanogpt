{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "class Attention(nn.Module): # single head for now, we'll add multi-head later!\n",
    "    def __init__(self, D=256): \n",
    "        super().__init__()\n",
    "        self.D = D\n",
    "        # divide by sqrt(D) to keep variance roughly constant, otherwise logits get too big\n",
    "        self.scale = torch.sqrt(torch.tensor(D, dtype=torch.float32))\n",
    "        # these are just linear projections that map input -> query/key/value vectors\n",
    "        self.wq = nn.Linear(D, D) # query projection\n",
    "        self.wk = nn.Linear(D, D) # key projection  \n",
    "        self.wv = nn.Linear(D, D) # value projection\n",
    "        self.wo = nn.Linear(D, D) # final output projection\n",
    "\n",
    "    def forward(self, x): # x is [B, S, D] (batch, sequence length, hidden dim)\n",
    "        # project input into Q,K,V vectors - each is [B, S, D]\n",
    "        Q, K, V = self.wq(x), self.wk(x), self.wv(x)\n",
    "        \n",
    "        # compute attention scores between each position - [B, S, D] @ [B, D, S] -> [B, S, S]\n",
    "        # scale to prevent softmax saturation which would kill gradients\n",
    "        A_logits = (Q @ K.transpose(1, 2))/self.scale\n",
    "        \n",
    "        # convert scores to probabilities with softmax - each query attends to all keys\n",
    "        A = F.softmax(A_logits, dim=-1) # [B, S, S]\n",
    "        \n",
    "        # weighted sum of values based on attention probs\n",
    "        # [B, S, S] @ [B, S, D] -> [B, S, D], then project back to output space\n",
    "        return self.wo(A@V)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
