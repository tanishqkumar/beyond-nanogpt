{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in ublock fwd, input x has shape torch.Size([4, 1, 32, 32])\n",
      "Input x shape: torch.Size([4, 1, 32, 32])\n",
      "After gn1 shape: torch.Size([4, 1, 32, 32])\n",
      "After silu1 shape: torch.Size([4, 1, 32, 32])\n",
      "After conv1 shape: torch.Size([4, 1, 17, 17])\n",
      "After gn2 shape: torch.Size([4, 1, 17, 17])\n",
      "After silu2 shape: torch.Size([4, 1, 17, 17])\n",
      "After conv2 shape: torch.Size([4, 1, 10, 10])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (10) must match the size of tensor b (15) at non-singleton dimension 3",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 257\u001b[0m\n\u001b[1;32m    255\u001b[0m model \u001b[38;5;241m=\u001b[39m UNet()\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m    256\u001b[0m T \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m100\u001b[39m \n\u001b[0;32m--> 257\u001b[0m model(torch\u001b[38;5;241m.\u001b[39mrandn(\u001b[38;5;241m4\u001b[39m,\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m32\u001b[39m,\u001b[38;5;241m32\u001b[39m)\u001b[38;5;241m.\u001b[39mto(device), torch\u001b[38;5;241m.\u001b[39mrandint(\u001b[38;5;241m0\u001b[39m,T,(\u001b[38;5;241m4\u001b[39m,),device\u001b[38;5;241m=\u001b[39mdevice))\u001b[38;5;241m.\u001b[39mshape\n",
      "File \u001b[0;32m~/.conda/envs/lingua_env/lib/python3.11/site-packages/torch/nn/modules/module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/.conda/envs/lingua_env/lib/python3.11/site-packages/torch/nn/modules/module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[0;32mIn[13], line 236\u001b[0m, in \u001b[0;36mUNet.forward\u001b[0;34m(self, x, t)\u001b[0m\n\u001b[1;32m    234\u001b[0m     ss_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtime_to_ss[layer_counter](t_embeds)  \u001b[38;5;66;03m# [b, 2*ch]\u001b[39;00m\n\u001b[1;32m    235\u001b[0m     scale, shift \u001b[38;5;241m=\u001b[39m ss_output\u001b[38;5;241m.\u001b[39mchunk(\u001b[38;5;241m2\u001b[39m, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)  \u001b[38;5;66;03m# split into two [b, ch] tensors\u001b[39;00m\n\u001b[0;32m--> 236\u001b[0m     h \u001b[38;5;241m=\u001b[39m down_block(h, scale, shift)\n\u001b[1;32m    237\u001b[0m     layer_counter \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    239\u001b[0m ss_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtime_to_ss[layer_counter](t_embeds)  \u001b[38;5;66;03m# [b, 2*ch]\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/lingua_env/lib/python3.11/site-packages/torch/nn/modules/module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/.conda/envs/lingua_env/lib/python3.11/site-packages/torch/nn/modules/module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[0;32mIn[13], line 201\u001b[0m, in \u001b[0;36mUBlock.forward\u001b[0;34m(self, x, scale, shift)\u001b[0m\n\u001b[1;32m    198\u001b[0m h2 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv2(h2)\n\u001b[1;32m    199\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAfter conv2 shape: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mh2\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 201\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m h2 \u001b[38;5;241m+\u001b[39m og_x\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (10) must match the size of tensor b (15) at non-singleton dimension 3"
     ]
    }
   ],
   "source": [
    "# let's implemet ddpm with a u-net with a score-matching (or equivalently, noise prediction) objective \n",
    "# same setup as in DiT but now with a convolutional backbone. Recall, we take noised_img [b, ch, h, w] -> noise_pred [b, ch, h, w]\n",
    "# with (eps_true - eps_pred).mean() as loss \n",
    "# new additions here: groupnorm, new type of timeEmbeddings, Ublock and Unet with bottleneck structure compared to DiT \n",
    "import torchvision \n",
    "import math \n",
    "import torch \n",
    "import torch.nn as nn \n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# this conv is more involved than in eg. our resnet implementation because we will be supporting arbitrary up/downsampling of inputs \n",
    "# but the core idea of converting a conv into a batched matmul is the same, just with casework to interpolate upsample before conv or \n",
    "# add a stride to downsample during the conv, conceptually not very different\n",
    "# and i'm sure i could've abstracted out some of the code to make this cleaner/shorter but nbd\n",
    "class Conv(nn.Module): # [b, ch, h, w] -> [b, ch', h', w'] where ch', h', w' depend on down/upsampling ratio \n",
    "    def __init__(self, upsample_ratio=1.0, kernel_sz=3, ch_in=1, ch_out=1): \n",
    "        super().__init__()\n",
    "        self.upsample_ratio = upsample_ratio\n",
    "        self.kernel_sz = kernel_sz\n",
    "        self.ch_in = ch_in\n",
    "        self.ch_out = ch_out\n",
    "        self.kernel_weights = torch.nn.Parameter(torch.randn(ch_out, ch_in, kernel_sz, kernel_sz))\n",
    "\n",
    "    def forward(self, x): # residual conv \n",
    "        if self.upsample_ratio > 1.0:\n",
    "            # interpolate up then conv to maintain \n",
    "            x = F.interpolate(x, scale_factor=self.upsample_ratio)\n",
    "\n",
    "            # conv to maintain \n",
    "            patches = F.unfold(x, kernel_size=self.kernel_sz, padding=self.kernel_sz//2) # [b, ch_in * k * k, np]\n",
    "            kernel_flat = self.kernel_weights.reshape(self.ch_out, -1) # [ch_out, ch_in * k * k]\n",
    "            out = torch.einsum('bmn,om->bon', patches, kernel_flat)# [b, ch_out, np]\n",
    "            b, _, h, w = x.shape\n",
    "            out = out.reshape(b, self.ch_out, h, w)\n",
    "            return out \n",
    "        elif self.upsample_ratio < 1.0:\n",
    "            # downsample by setting stride=1//upsample_ratio\n",
    "            stride = int(1/self.upsample_ratio)\n",
    "            padding = int(1/self.upsample_ratio)\n",
    "\n",
    "            patches = F.unfold(x, kernel_size=self.kernel_sz, padding=padding, stride=stride) # [b, ch_in * k * k, np]\n",
    "            kernel_flat = self.kernel_weights.reshape(self.ch_out, -1) # [ch_out, ch_in * k * k]\n",
    "            out = torch.einsum('bmn,om->bon', patches, kernel_flat)# [b, ch_out, np]\n",
    "            b, _, h, w = x.shape\n",
    "\n",
    "            h_out = (h + 2*padding - self.kernel_sz)//stride + 1\n",
    "            w_out = (w + 2*padding - self.kernel_sz)//stride + 1\n",
    "            out = out.reshape(b, self.ch_out, h_out, w_out)\n",
    "            return out \n",
    "        else: # == 1 \n",
    "            # this conv maintains dimensionality\n",
    "            patches = F.unfold(x, kernel_size=self.kernel_sz, padding=self.kernel_sz//2) # [b, ch_in * k * k, np]\n",
    "            kernel_flat = self.kernel_weights.reshape(self.ch_out, -1) # [ch_out, ch_in * k * k]\n",
    "            out = torch.einsum('bmn,om->bon', patches, kernel_flat)# [b, ch_out, np]\n",
    "            b, _, h, w = x.shape\n",
    "            out = out.reshape(b, self.ch_out, h, w)\n",
    "            return out \n",
    "\n",
    "\n",
    "class GroupNorm(nn.Module): \n",
    "    def __init__(self, ch=3, channels_per_group=3, eps = 1e-8): \n",
    "        super().__init__()\n",
    "        if channels_per_group > ch: \n",
    "            channels_per_group = ch \n",
    "        self.channels_per_group = channels_per_group\n",
    "        assert ch % channels_per_group == 0, \\\n",
    "            \"GroupNorm requires number of channels to be a multiple of channels_per_group!\"\n",
    "        self.num_groups = int(ch/channels_per_group)\n",
    "        self.shift = nn.Parameter(torch.zeros(self.num_groups))\n",
    "        self.scale = nn.Parameter(torch.ones(self.num_groups))\n",
    "        self.eps = eps \n",
    "\n",
    "    \n",
    "    def forward(self, x): # [b, ch, h, w] -> [b, ch, h, w] \n",
    "        # reshape to [b, ch//g, g * h * w], normalize within the last, and reshape back \n",
    "        # g is number of groups \n",
    "        b, ch, h, w = x.shape\n",
    "        out = x.reshape(b, self.num_groups, self.channels_per_group * h * w)\n",
    "        out = (out - out.mean(dim=-1, keepdim=True))/(out.std(dim=-1, keepdim=True) + self.eps)\n",
    "        # broadcast from [ng] to [1, ng, 1] to can mult with [b, ng, cpg * h * w]\n",
    "        shift_reshaped = self.shift.view(1, self.num_groups, 1)\n",
    "        scale_reshaped = self.scale.view(1, self.num_groups, 1)\n",
    "        out = (out + shift_reshaped) * scale_reshaped  # apply to each group in each batch separately\n",
    "        return out.reshape(b, ch, h, w)\n",
    "\n",
    "class Attention(nn.Module): \n",
    "    def __init__(self, D=1): # ch = 1 for mnist, ch=3 for cifar-10\n",
    "        super().__init__()\n",
    "        self.D = D\n",
    "        self.wq = nn.Linear(D, D)\n",
    "        self.wk = nn.Linear(D, D)\n",
    "        self.wv = nn.Linear(D, D)\n",
    "        self.wo = nn.Linear(D, D)\n",
    "    \n",
    "    def forward(self, x): \n",
    "        # fwd is [b, ch, h, w] -> [b, ch, h * w] -> then \n",
    "        # self attn across last dim with b, ch as batch dims, reshape to [b, ch, h, w]\n",
    "        b, ch, h, w = x.shape\n",
    "        # [b, h*w, ch] like in a transformer now [b,s,d] with s = h*w tokens and ch = features (D)\n",
    "        x = x.reshape(b, h * w, ch).transpose(-1, -2) # [b, ch, h * w]\n",
    "        q, k, v = self.wq(x), self.wk(x), self.wv(x) # [b, s, d]\n",
    "\n",
    "        scale = math.sqrt(self.D)\n",
    "        A_logits = torch.bmm(q/scale, k.transpose(-1, -2))\n",
    "        A = F.softmax(A_logits, dim=-1) # [b, s, s]\n",
    "\n",
    "        out = torch.bmm(A, v) # [b, s, s] @ [b, s, d] -> [b, s, d]\n",
    "        out = self.wo(out) # [b, s, d] = [b, h * w, ch]\n",
    "        return out.transpose(-1, -2).reshape(b, ch, h, w) # output [b, ch, h, w]\n",
    "        \n",
    "class TimeEmbedding(nn.Module): # \n",
    "    def __init__(self, dim, max_period=10000, mlp_mult=4):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.max_period = max_period # controls frequency of our sinusoidal embeddings\n",
    "        \n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(dim, dim * mlp_mult), \n",
    "            nn.SiLU() ,\n",
    "            nn.Linear(dim * mlp_mult, dim), \n",
    "        )\n",
    "    \n",
    "    def forward(self, t): # [b] -> [b, dim] where then projection to [b, 2*ch] handled in UNet class\n",
    "        # sinusoidal embeddings, freqs shape: [dim//2]\n",
    "        half_dim = self.dim // 2\n",
    "        freqs = torch.exp(\n",
    "            -torch.arange(half_dim, device=t.device) * torch.log(torch.tensor(self.max_period)) / half_dim\n",
    "        )\n",
    "        \n",
    "        args = t[:,None] * freqs[None, :] # [b, dim//2]\n",
    "        embedding = torch.cat([torch.cos(args), torch.sin(args)], dim=-1)  # [b, dim]\n",
    "        \n",
    "        # handle odd dim\n",
    "        if self.dim % 2 == 1:\n",
    "            embedding = torch.cat([embedding, torch.zeros_like(embedding[:, :1])], dim=-1)\n",
    "            \n",
    "        return self.mlp(embedding)\n",
    "\n",
    "class UBlock(nn.Module):\n",
    "    # 2x [GN, +embeddings, SiLU, Conv] with global residual \n",
    "    def __init__(self, ch_in, up=True, bottleneck=False, k=3): # gamma, beta are both [b, ch] vectors from TimeEmbeddings that we use to steer things \n",
    "        super().__init__()\n",
    "        # bottleneck means 1x1 + self attn \n",
    "        self.up = up \n",
    "        self.gn1 = GroupNorm(ch=ch_in)\n",
    "        self.silu1 = nn.SiLU()\n",
    "        self.gn2 = GroupNorm(ch=ch_in)\n",
    "        self.silu2 = nn.SiLU()\n",
    "\n",
    "        self.bottleneck = bottleneck\n",
    "\n",
    "        if self.up: # upsample by 2\n",
    "            self.conv1 = Conv(ch_in=ch_in, ch_out=ch_in, upsample_ratio=2, kernel_sz=k)\n",
    "            self.conv2 = Conv(ch_in=ch_in, ch_out=ch_in, upsample_ratio=2, kernel_sz=k)\n",
    "        elif bottleneck: # preserve shape and use attn\n",
    "            self.attn = Attention(D=ch_in)\n",
    "            self.conv1 = Conv(ch_in=ch_in, ch_out=ch_in, upsample_ratio=1, kernel_sz=k)\n",
    "            self.conv2 = Conv(ch_in=ch_in, ch_out=ch_in, upsample_ratio=1, kernel_sz=k)\n",
    "        else: # downsample by 2\n",
    "            self.conv1 = Conv(ch_in=ch_in, ch_out=ch_in, upsample_ratio=0.5, kernel_sz=k)\n",
    "            self.conv2 = Conv(ch_in=ch_in, ch_out=ch_in, upsample_ratio=0.5, kernel_sz=k)\n",
    "            self.skip = nn.Conv2d(in_channels=ch_in, out_channels=ch_in, kernel_size=k, stride=2)\n",
    "\n",
    "    def forward(self, x, scale, shift): # conv, norm, and skip connection, with optional self-attn\n",
    "        b, ch, h, w = x.shape \n",
    "        og_x = x.clone()\n",
    "        if self.up: \n",
    "            og_x = F.interpolate(og_x, scale_factor=2)\n",
    "        elif not self.up and not self.bottleneck: # self.down \n",
    "            og_x = self.skip(og_x)\n",
    "\n",
    "        print(f'in ublock fwd, input x has shape {x.shape}')\n",
    "\n",
    "        scale = scale.view(b, ch, 1, 1)\n",
    "        shift = shift.view(b, ch, 1, 1)\n",
    "        \n",
    "        print(f\"Input x shape: {x.shape}\")\n",
    "        h1 = self.gn1(x) * scale + shift\n",
    "        print(f\"After gn1 shape: {h1.shape}\")\n",
    "        h1 = self.silu1(h1)\n",
    "        print(f\"After silu1 shape: {h1.shape}\")\n",
    "        if self.bottleneck: \n",
    "            h1 = self.attn(h1) # attn preserves [b, ch, h, w] shape\n",
    "            print(f\"After attn shape: {h1.shape}\")\n",
    "        h1 = self.conv1(h1)\n",
    "        print(f\"After conv1 shape: {h1.shape}\")\n",
    "\n",
    "        h2 = self.gn2(h1) * scale + shift\n",
    "        print(f\"After gn2 shape: {h2.shape}\")\n",
    "        h2 = self.silu2(h2)\n",
    "        print(f\"After silu2 shape: {h2.shape}\")\n",
    "        if self.bottleneck: \n",
    "            h2 = self.attn(h2)\n",
    "            print(f\"After attn shape: {h2.shape}\")\n",
    "        h2 = self.conv2(h2)\n",
    "        print(f\"After conv2 shape: {h2.shape}\")\n",
    "        \n",
    "        return h2 + og_x\n",
    "        \n",
    "\n",
    "class UNet(nn.Module): # [b, ch, h, w] noised image -> [b, ch, h, w] of error (score-matching)\n",
    "    def __init__(self, nblocks=11, time_embed_dim=64, ch=1, h=32, w=32, k=3): # 5 down, 1 bottleneck, 5 up \n",
    "        super().__init__()\n",
    "        self.nblocks = nblocks\n",
    "        self.time_embed_dim = time_embed_dim\n",
    "        self.ch = ch\n",
    "        self.h = h\n",
    "        self.w = w\n",
    "        \n",
    "        self.time_embeddings = TimeEmbedding(self.time_embed_dim)\n",
    "        \n",
    "        self.ups = nn.Sequential(\n",
    "            *[UBlock(ch, up=True, bottleneck=False, k=k) for _ in range(int(nblocks//2))]\n",
    "        )\n",
    "        self.bottleneck = UBlock(ch, up=False, bottleneck=True, k=k)\n",
    "        self.downs = nn.Sequential(\n",
    "            *[UBlock(ch, up=False, bottleneck=False, k=k) for _ in range(int(nblocks//2))]\n",
    "        )\n",
    "        \n",
    "        self.time_to_ss = nn.ModuleList([nn.Linear(time_embed_dim, 2 * ch) for _ in range(nblocks)])\n",
    "\n",
    "\n",
    "    def forward(self, x, t): # [b, ch, h, w] -> [b, ch, h, w]\n",
    "        b = x.shape[0]\n",
    "        h = x\n",
    "        # h = F.pad(x, (2, 2, 2, 2), mode='constant', value=0) # [b, 1, 28, 28] -> [b, 1, 32, 32]\n",
    "        t_embeds = self.time_embeddings(t) # add to every UBlock \n",
    "        layer_counter = 0 \n",
    "\n",
    "        for down_block in self.downs: \n",
    "            ss_output = self.time_to_ss[layer_counter](t_embeds)  # [b, 2*ch]\n",
    "            scale, shift = ss_output.chunk(2, dim=1)  # split into two [b, ch] tensors\n",
    "            h = down_block(h, scale, shift)\n",
    "            layer_counter += 1\n",
    "        \n",
    "        ss_output = self.time_to_ss[layer_counter](t_embeds)  # [b, 2*ch]\n",
    "        scale, shift = ss_output.chunk(2, dim=1)  # split into two [b, ch] tensors\n",
    "        h = self.bottleneck(h, scale, shift)\n",
    "        layer_counter += 1\n",
    "\n",
    "        for up_block in self.ups: \n",
    "            ss_output = self.time_to_ss[layer_counter](t_embeds)  # [b, 2*ch]\n",
    "            scale, shift = ss_output.chunk(2, dim=1)  # split into two [b, ch] tensors\n",
    "            h = up_block(h, scale, shift)\n",
    "            layer_counter += 1\n",
    "\n",
    "        return h # [b, ch, h, w]\n",
    "\n",
    "    pass # Ublock(down) x N -> Ublock(up) x N with middle layers having attn \n",
    "\n",
    "device = 'cuda'\n",
    "model = UNet().to(device)\n",
    "T = 100 \n",
    "model(torch.randn(4,1,32,32).to(device), torch.randint(0,T,(4,),device=device)).shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# t is [b]\n",
    "# freqs is [dim//2]\n",
    "# t[:,None] * freqs[None, :] is [b, dim//2]\n",
    "# we cat these two along dim=-1 to get [b, dim] output from TimeEmbeddings\n",
    "# UNet class stores projections [b, dim] -> [b, 2*ch] for each UBlock \n",
    "# we compute t_embed = TimeEmbedding(t) once at the beginning\n",
    "# then does shift_i, scale_i = projs[i](t_embeds) \n",
    "# and passes in shift_i and scale_i to UBlocks[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: noise schedule, construct every batch element with distinct noise \n",
    "# ie. one el might have x_t the other might have x_{s>t} in terms of noise\n",
    "# ie. will need alphas_cumprod, etc. \n",
    "def train(model, dataloader, betas, b=16, ch=1, h=28, w=28, epochs=1, lr=3e-4, print_every_steps=10, T=200): # inputs are both [b, ch, h, w], latter is unet(real_batch + true_noise)\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    opt = torch.optim.AdamW(model.parameters(), lr=lr)\n",
    "    opt.zero_grad()\n",
    "\n",
    "    t = torch.randint(0, T, (b,))\n",
    "    # \n",
    "    \n",
    "    step = 0\n",
    "    for epoch_idx in range(epochs): \n",
    "        for real_batch, _ in dataloader: \n",
    "            real_batch = real_batch.to(device=device)\n",
    "            true_noise = torch.randn(b, ch, h, w) # TODO: need diff variance noise for each batch el based on time steps T choose b\n",
    "            pred_noise = model(real_batch + true_noise, t) # needs t for time step embeddings \n",
    "            loss = F.mse(true_noise, pred_noise)\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "            opt.zero_grad()\n",
    "\n",
    "            step +=1\n",
    "            if step % print_every_steps == 0: \n",
    "                print(f'Step {step}, epoch {epoch_idx}: Loss {loss.item()}')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"main\": \n",
    "\n",
    "    # put all the other argparse stuff for real training here \n",
    "    betas = 0 \n",
    "    batch_sz = 64\n",
    "\n",
    "    model = UNet()\n",
    "    dataloader = torch.utils.data.DataLoader(\n",
    "        torchvision.datasets.MNIST(\n",
    "            root='./data',\n",
    "            train=True,\n",
    "            download=True,\n",
    "            transform=torchvision.transforms.ToTensor()\n",
    "        ),\n",
    "        batch_size=batch_sz,\n",
    "        shuffle=True\n",
    "    )\n",
    "    \n",
    "    train(model, dataloader, betas, b=batch_sz)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 1, 1024])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 1, 32, 32])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch \n",
    "import torch.nn as nn \n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# this conv maintains dimensionality\n",
    "\n",
    "b, ch_in, ch_out, h, w, k = 16, 3, 1, 32, 32, 3\n",
    "kernel_weights = torch.randn(ch_in, ch_out, k, k)\n",
    "x = torch.randn(b, ch_in, h, w)\n",
    "\n",
    "# to do all matmuls in parallel\n",
    "patches = F.unfold(x, kernel_size=k, padding=k//2) # [b, ch_in * k * k, np]\n",
    "kernel_flat = kernel_weights.reshape(ch_out, -1) # [ch_out, ch_in * k * k]\n",
    "out = torch.einsum('bmn,om->bon', patches, kernel_flat)# [b, ch_out, np]\n",
    "out.reshape(b, ch_out, h, w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "shape '[1, 32, 32]' is invalid for input of size 13456",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m kernel_flat \u001b[38;5;241m=\u001b[39m kernel_weights\u001b[38;5;241m.\u001b[39mreshape(ch_out, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m) \u001b[38;5;66;03m# [ch_out, ch_in * k * k]\u001b[39;00m\n\u001b[1;32m      4\u001b[0m out \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39meinsum(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbmn,om->bon\u001b[39m\u001b[38;5;124m'\u001b[39m, patches, kernel_flat)\u001b[38;5;66;03m# [b, ch_out, np]\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m out\u001b[38;5;241m.\u001b[39mreshape(ch_out, h, w)\u001b[38;5;241m.\u001b[39mshape\n",
      "\u001b[0;31mRuntimeError\u001b[0m: shape '[1, 32, 32]' is invalid for input of size 13456"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "envi",
   "language": "python",
   "name": "lingua_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
